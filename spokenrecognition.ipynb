{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled17.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "TPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Kanaagalakshmi/Mavericks/blob/master/spokenrecognition.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Mys8pXLDqV8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TFRYqkGhDNaW",
        "colab_type": "code",
        "outputId": "a2fe89de-ccb1-4e2f-c90c-42c155e5d79d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 207
        }
      },
      "source": [
        "import os\n",
        "import time\n",
        "import random\n",
        "!pip install numpy==1.14.3\n",
        "!pip install librosa==0.6.1\n",
        "!pip install tensorflow==1.8.0\n",
        "!pip install TFLearn\n",
        "import tflearn\n",
        "\n",
        "import numpy as np\n",
        "import librosa.display\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt\n",
        "tf.reset_default_graph()\n",
        "# EXTRACT MFCC FEATURES\n",
        "#\n",
        "def extract_mfcc(file_path, utterance_length):\n",
        "    # Get raw .wav data and sampling rate from librosa's load function\n",
        "    raw_w, sampling_rate = librosa.load(file_path+\"/\", mono=True)\n",
        "\n",
        "    # Obtain MFCC Features from raw data\n",
        "    mfcc_features = librosa.feature.mfcc(raw_w, sampling_rate)\n",
        "    if mfcc_features.shape[1] > utterance_length:\n",
        "        mfcc_features = mfcc_features[:, 0:utterance_length]\n",
        "    else:\n",
        "        mfcc_features = np.pad(mfcc_features, ((0, 0), (0, utterance_length - mfcc_features.shape[1])),\n",
        "                               mode='constant', constant_values=0)\n",
        "    \n",
        "    return mfcc_features\n",
        "\n",
        "#\n",
        "# GET TRAINING BATCH, returns data in batches \n",
        "#\n",
        "def get_mfcc_batch(file_path, batch_size, utterance_length):\n",
        "    print(\"hello\")\n",
        "    files = os.listdir(file_path)\n",
        "    ft_batch = []\n",
        "    label_batch = []\n",
        "\n",
        "    while True:\n",
        "        # Shuffle Files\n",
        "        random.shuffle(files)\n",
        "        for fname in files:\n",
        "            # print(\"Total %d files in directory\" % len(files))\n",
        "\n",
        "            # Make sure file is a .wav file\n",
        "            if not fname.endswith(\".wav\"):\n",
        "                continue\n",
        "            \n",
        "            # Get MFCC Features for the file\n",
        "            mfcc_features = extract_mfcc(file_path +\"/\"+fname, utterance_length)\n",
        "            \n",
        "            # One-hot encode label for 10 digits 0-9\n",
        "            label = np.eye(10)[int(fname[0])]\n",
        "            \n",
        "            # Append to label batch\n",
        "            label_batch.append(label)\n",
        "            \n",
        "            # Append mfcc features to ft_batch\n",
        "            ft_batch.append(mfcc_features)\n",
        "\n",
        "            # Check to see if default batch size is < than ft_batch\n",
        "            if len(ft_batch) >= batch_size:\n",
        "                # send over batch\n",
        "                yield ft_batch, label_batch\n",
        "                # reset batches\n",
        "                ft_batch = []\n",
        "                labels_batch = []\n",
        "\n",
        "#\n",
        "# DISPLAY FEATURE SHAPE\n",
        "#\n",
        "# wav_file_path: Input a file path to a .wav file\n",
        "#\n",
        "def display_power_spectrum(wav_file_path, utterance_length):\n",
        "    mfcc = extract_mfcc(wav_file_path, utterance_length)\n",
        "    \n",
        "    # Plot\n",
        "    plt.figure(figsize=(10, 6))\n",
        "    plt.subplot(2, 1, 1)\n",
        "    librosa.display.specshow(mfcc, x_axis='time')\n",
        "    plt.show()\n",
        "\n",
        "    # Feature information\n",
        "    print(\"Feature Shape: \", mfcc.shape)\n",
        "    print(\"Features: \" , mfcc[:,0])\n",
        "\n",
        "#\n",
        "# MAIN\n",
        "#\n",
        "def main():\n",
        "    # Initial Parameters\n",
        "    lr = 0.001\n",
        "    iterations_train = 50\n",
        "    bsize = 64\n",
        "    audio_features = 20  \n",
        "    utterance_length = 35  # Modify to see what different results you can get\n",
        "    ndigits = 10\n",
        "\n",
        "    # Get training data\n",
        "    train_batch = get_mfcc_batch('/content/drive/My Drive/new spoken digit/Spoken-Digit-Recognition/new_spoken_digit/dataset', 150, utterance_length)\n",
        "    \n",
        "    # # Build Model\n",
        "    sp_network = tflearn.input_data([None, audio_features, utterance_length])\n",
        "    sp_network = tflearn.lstm(sp_network, 128*4, dropout=0.5)\n",
        "    sp_network = tflearn.fully_connected(sp_network, ndigits, activation='softmax')\n",
        "    sp_network = tflearn.regression(sp_network, optimizer='adam', learning_rate=lr, loss='categorical_crossentropy')\n",
        "    sp_model = tflearn.DNN(sp_network, tensorboard_verbose=0)\n",
        "\n",
        "    # Train Model\n",
        "    while iterations_train > 0:\n",
        "        X_tr, y_tr = next(train_batch)\n",
        "        X_test, y_test = next(train_batch)\n",
        "        sp_model.fit(X_tr, y_tr, n_epoch=10, validation_set=(X_test, y_test), show_metric=True, batch_size=bsize)\n",
        "        iterations_train -=1\n",
        "    sp_model.save(\"/content/drive/My Drive/new spoken digit/Spoken-Digit-Recognition/new_spoken_digit/lstm files/speech_recognition.lstm.index\")\n",
        "\n",
        "    # Test Model\n",
        "    sp_model.load('/content/drive/My Drive/new spoken digit/Spoken-Digit-Recognition/new_spoken_digit/lstm files/speech_recognition.lstm.index')\n",
        "    mfcc_features = extract_mfcc('/content/drive/My Drive/test/0_jackson_49.wav', utterance_length)\n",
        "    mfcc_features = mfcc_features.reshape((1,mfcc_features.shape[0],mfcc_features.shape[1]))\n",
        "    prediction_digit = sp_model.predict(mfcc_features)\n",
        "    print(prediction_digit)\n",
        "    print(\"Digit predicted: \", np.argmax(prediction_digit))\n",
        "\n",
        "    # Done\n",
        "    return 0\n",
        "\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training Step: 1499  | total loss: \u001b[1m\u001b[32m2.29775\u001b[0m\u001b[0m | time: 0.622s\n",
            "| Adam | epoch: 500 | loss: 2.29775 - acc: 0.1333 -- iter: 128/150\n",
            "Training Step: 1500  | total loss: \u001b[1m\u001b[32m2.28910\u001b[0m\u001b[0m | time: 2.012s\n",
            "| Adam | epoch: 500 | loss: 2.28910 - acc: 0.1434 | val_loss: 2.31813 - val_acc: 0.1067 -- iter: 150/150\n",
            "--\n",
            "INFO:tensorflow:/content/drive/My Drive/new spoken digit/Spoken-Digit-Recognition/new_spoken_digit/lstm files/speech_recognition.lstm.index is not in all_model_checkpoint_paths. Manually adding it.\n",
            "INFO:tensorflow:Restoring parameters from /content/drive/My Drive/new spoken digit/Spoken-Digit-Recognition/new_spoken_digit/lstm files/speech_recognition.lstm.index\n",
            "[[0.10514367 0.08418491 0.09734729 0.08213335 0.08085749 0.12703276\n",
            "  0.11499744 0.1152242  0.1216312  0.07144772]]\n",
            "Digit predicted:  5\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}